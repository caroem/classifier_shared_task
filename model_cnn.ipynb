{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Embedder(object):\n",
    "    def __init__(self, max_length=100):\n",
    "        self.max_length = max_length\n",
    "        print(\"Loading spacy...\")\n",
    "        self.nlp = spacy.load('en')\n",
    "        print(\"...done\")\n",
    "\n",
    "    def embed(self, text):\n",
    "        return self._pad(self._vectors(text))\n",
    "\n",
    "    def shape(self):\n",
    "        return (self.max_length, 300)\n",
    "\n",
    "    def _vectors(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        vectors = []\n",
    "        for token in doc:\n",
    "            vectors.append(token.vector)\n",
    "        return vectors\n",
    "\n",
    "    def _pad(self, vectors):\n",
    "        vector_dim = len(vectors[0])\n",
    "        sequence = np.zeros((self.max_length, vector_dim))\n",
    "        for i, vector in enumerate(vectors):\n",
    "            if i == self.max_length:\n",
    "                break\n",
    "            sequence[i] = vector\n",
    "        return sequence\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(self, embedder, model_path, prompt_names_path):\n",
    "        self.model = load_model(model_path)\n",
    "        self.prompt_names = pickle.load(open(prompt_names_path, \"rb\"))\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def predict(self, text):\n",
    "        proba = self._predict_proba(text)\n",
    "        return self.prompt_names[np.argmax(proba)]\n",
    "\n",
    "    def _predict_proba(self, text):\n",
    "        embedding = self.embedder.embed(text)\n",
    "        return self.model.predict(np.array([embedding]))\n",
    "\n",
    "def predict(model, text):\n",
    "    embedding = embedder.embed(text)\n",
    "    return model.predict(np.array([embedding]))\n",
    "\n",
    "\n",
    "def translate(prediction, prompt_names):\n",
    "    return prompt_names[np.argmax(prediction)]\n",
    "\n",
    "\n",
    "def suggest(text):\n",
    "    prediction = predict(loaded_model, text)\n",
    "    return translate(prediction, loaded_prompt_names)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    prompt_names = {}\n",
    "    with open(data_dir + '/01.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "    for i, prompt in enumerate(data):\n",
    "        prompt_names[i] = prompt[\"prompt\"]\n",
    "        for message in prompt[\"response\"]:\n",
    "            examples.append((message, i))\n",
    "    np.random.shuffle(examples)\n",
    "    #if limit >= 1:\n",
    "    #    examples = examples[:limit]\n",
    "    messages, prompts = zip(*examples)\n",
    "    return examples, prompt_names\n",
    "\n",
    "def dummy_encode(array, num_classes=None):\n",
    "    array = np.array(array)\n",
    "    if num_classes is None:\n",
    "        num_classes = max(array) + 1\n",
    "    result = np.zeros((len(array), num_classes))\n",
    "    result[np.arange(len(array)), array] = 1\n",
    "    return result\n",
    "\n",
    "def create_dataset(type):\n",
    "    if type == 'train':\n",
    "        data_dir = 'data/train'\n",
    "    elif type == 'dev':\n",
    "        data_dir = 'data/dev'\n",
    "    examples, prompt_names = load_data(data_dir)\n",
    "    X = []\n",
    "    y = []\n",
    "    for example in examples:\n",
    "        message, prompt = example[0], example[1]\n",
    "        X.append(embedder.embed(message))\n",
    "        y.append(prompt)\n",
    "    return np.array(X), dummy_encode(np.array(y)), prompt_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy...\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "embedder = Embedder()\n",
    "X_train, y_train, prompt_names_train = create_dataset('train')\n",
    "X_dev, y_dev, prompt_names_dev = create_dataset('dev')\n",
    "X, y, prompt_names = X_train, y_train, prompt_names_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    sequences = Input(shape=input_shape)\n",
    "    x = Conv1D(400, 5, activation='relu')(sequences)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(400, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(400, activation='relu')(x)\n",
    "    preds = Dense(len(prompt_names), activation='softmax')(x)\n",
    "    model = Model(input=sequences, output=preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def build_model2(input_shape):\n",
    "    sequences = Input(shape=input_shape)\n",
    "    x = Conv1D(512, 5, activation='relu')(sequences)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(512, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    preds = Dense(len(prompt_names), activation='softmax')(x)\n",
    "    model = Model(input=sequences, output=preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def build_model3(input_shape):\n",
    "    sequences = Input(shape=input_shape)\n",
    "    x = Conv1D(400, 5, activation='relu')(sequences)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(400, 2, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(400, activation='relu')(x)\n",
    "    preds = Dense(len(prompt_names), activation='softmax')(x)\n",
    "    model = Model(input=sequences, output=preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    model.save('output/' + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(prompt_names, open(\"output/prompt_names.p\", \"wb\"))\n",
    "loaded_prompt_names = pickle.load(open(\"output/prompt_names.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26408 samples, validate on 8794 samples\n",
      "Epoch 1/40\n",
      "26408/26408 [==============================] - 11s - loss: 4.5642 - acc: 0.0526 - val_loss: 3.8094 - val_acc: 0.1071\n",
      "Epoch 2/40\n",
      "26408/26408 [==============================] - 10s - loss: 3.0429 - acc: 0.1893 - val_loss: 2.9008 - val_acc: 0.2319\n",
      "Epoch 3/40\n",
      "26408/26408 [==============================] - 10s - loss: 2.1049 - acc: 0.3804 - val_loss: 1.7825 - val_acc: 0.4775\n",
      "Epoch 4/40\n",
      "26408/26408 [==============================] - 10s - loss: 1.4156 - acc: 0.5802 - val_loss: 1.2680 - val_acc: 0.6257\n",
      "Epoch 5/40\n",
      "26408/26408 [==============================] - 10s - loss: 1.0044 - acc: 0.7371 - val_loss: 0.7410 - val_acc: 0.8410\n",
      "Epoch 6/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.7395 - acc: 0.8260 - val_loss: 0.5572 - val_acc: 0.8780\n",
      "Epoch 7/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.6071 - acc: 0.8635 - val_loss: 0.4789 - val_acc: 0.8947\n",
      "Epoch 8/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.5658 - acc: 0.8784 - val_loss: 0.4187 - val_acc: 0.9139\n",
      "Epoch 9/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.4168 - acc: 0.9081 - val_loss: 0.6040 - val_acc: 0.8461\n",
      "Epoch 10/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.4482 - acc: 0.9053 - val_loss: 0.3190 - val_acc: 0.9343\n",
      "Epoch 11/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.3261 - acc: 0.9310 - val_loss: 0.2839 - val_acc: 0.9422\n",
      "Epoch 12/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.3416 - acc: 0.9322 - val_loss: 0.2865 - val_acc: 0.9418\n",
      "Epoch 13/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2956 - acc: 0.9387 - val_loss: 0.2352 - val_acc: 0.9521\n",
      "Epoch 14/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2662 - acc: 0.9449 - val_loss: 0.2164 - val_acc: 0.9555\n",
      "Epoch 15/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2062 - acc: 0.9550 - val_loss: 1.6023 - val_acc: 0.6550\n",
      "Epoch 16/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2177 - acc: 0.9555 - val_loss: 0.1823 - val_acc: 0.9619\n",
      "Epoch 17/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2285 - acc: 0.9542 - val_loss: 0.1761 - val_acc: 0.9633\n",
      "Epoch 18/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2300 - acc: 0.9556 - val_loss: 0.3346 - val_acc: 0.9155\n",
      "Epoch 19/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1618 - acc: 0.9654 - val_loss: 0.1705 - val_acc: 0.9650\n",
      "Epoch 20/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.2064 - acc: 0.9602 - val_loss: 0.1610 - val_acc: 0.9683\n",
      "Epoch 21/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1474 - acc: 0.9686 - val_loss: 0.1770 - val_acc: 0.9609\n",
      "Epoch 22/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1960 - acc: 0.9622 - val_loss: 0.1450 - val_acc: 0.9671\n",
      "Epoch 23/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1898 - acc: 0.9611 - val_loss: 0.1503 - val_acc: 0.9673\n",
      "Epoch 24/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1301 - acc: 0.9722 - val_loss: 0.1463 - val_acc: 0.9682\n",
      "Epoch 25/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1512 - acc: 0.9673 - val_loss: 0.1459 - val_acc: 0.9673\n",
      "Epoch 26/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1179 - acc: 0.9735 - val_loss: 0.1322 - val_acc: 0.9705\n",
      "Epoch 27/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1237 - acc: 0.9730 - val_loss: 0.1294 - val_acc: 0.9733\n",
      "Epoch 28/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1521 - acc: 0.9679 - val_loss: 0.1254 - val_acc: 0.9720\n",
      "Epoch 29/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0996 - acc: 0.9765 - val_loss: 0.1384 - val_acc: 0.9708\n",
      "Epoch 30/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0957 - acc: 0.9763 - val_loss: 0.1226 - val_acc: 0.9728\n",
      "Epoch 31/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1061 - acc: 0.9747 - val_loss: 0.1509 - val_acc: 0.9659\n",
      "Epoch 32/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1064 - acc: 0.9758 - val_loss: 0.6451 - val_acc: 0.8871\n",
      "Epoch 33/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1141 - acc: 0.9732 - val_loss: 0.1085 - val_acc: 0.9741\n",
      "Epoch 34/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1017 - acc: 0.9763 - val_loss: 0.1154 - val_acc: 0.9732\n",
      "Epoch 35/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.1004 - acc: 0.9769 - val_loss: 0.1052 - val_acc: 0.9752\n",
      "Epoch 36/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0878 - acc: 0.9784 - val_loss: 0.1027 - val_acc: 0.9756\n",
      "Epoch 37/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0990 - acc: 0.9769 - val_loss: 0.3118 - val_acc: 0.9345\n",
      "Epoch 38/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0809 - acc: 0.9789 - val_loss: 0.1057 - val_acc: 0.9742\n",
      "Epoch 39/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0934 - acc: 0.9777 - val_loss: 0.1015 - val_acc: 0.9754\n",
      "Epoch 40/40\n",
      "26408/26408 [==============================] - 10s - loss: 0.0919 - acc: 0.9763 - val_loss: 0.1031 - val_acc: 0.9753\n"
     ]
    }
   ],
   "source": [
    "model = build_model((X.shape[1], X.shape[2]))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), nb_epoch=40, batch_size=256)\n",
    "save_model(model, 'mymodel')\n",
    "embedder = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_model = load_model('output/mymodel')\n",
    "predictor1 = Predictor(embedder, 'output/mymodel', 'output/prompt_names.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sag: Ich möchte mit Kreditkarte bezahlen Sag: Ich möchte mit Visa bezahlen Sag: Ich möchte mit Dollars bezahlen Sag: Ich möchte mit Kreditkarte bezahlen\n"
     ]
    }
   ],
   "source": [
    "print(predictor1.predict('i want to pay with by credit card'),\n",
    "predictor1.predict('can i buy by post'),\n",
    "predictor1.predict('i would pay by dollars'),\n",
    "predictor1.predict('can i am a pay with credit card'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26408 samples, validate on 8794 samples\n",
      "Epoch 1/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0693 - acc: 0.9817 - val_loss: 0.1000 - val_acc: 0.9777\n",
      "Epoch 2/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0721 - acc: 0.9810 - val_loss: 0.0965 - val_acc: 0.9765\n",
      "Epoch 3/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0762 - acc: 0.9808 - val_loss: 0.1070 - val_acc: 0.9784\n",
      "Epoch 4/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0605 - acc: 0.9824 - val_loss: 0.1020 - val_acc: 0.9766\n",
      "Epoch 5/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0927 - acc: 0.9784 - val_loss: 0.1097 - val_acc: 0.9724\n",
      "Epoch 6/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0506 - acc: 0.9854 - val_loss: 0.1097 - val_acc: 0.9752\n",
      "Epoch 7/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0527 - acc: 0.9846 - val_loss: 0.4750 - val_acc: 0.9182\n",
      "Epoch 8/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0732 - acc: 0.9817 - val_loss: 0.0854 - val_acc: 0.9787\n",
      "Epoch 9/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0506 - acc: 0.9849 - val_loss: 0.0936 - val_acc: 0.9796\n",
      "Epoch 10/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0474 - acc: 0.9852 - val_loss: 0.0917 - val_acc: 0.9775\n",
      "Epoch 11/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0710 - acc: 0.9824 - val_loss: 0.0975 - val_acc: 0.9770\n",
      "Epoch 12/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0499 - acc: 0.9850 - val_loss: 0.0927 - val_acc: 0.9776\n",
      "Epoch 13/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0627 - acc: 0.9829 - val_loss: 0.0868 - val_acc: 0.9794\n",
      "Epoch 14/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0435 - acc: 0.9868 - val_loss: 0.0983 - val_acc: 0.9782\n",
      "Epoch 15/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0463 - acc: 0.9859 - val_loss: 0.0887 - val_acc: 0.9777\n",
      "Epoch 16/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0491 - acc: 0.9842 - val_loss: 0.0833 - val_acc: 0.9792\n",
      "Epoch 17/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0441 - acc: 0.9863 - val_loss: 0.0873 - val_acc: 0.9785\n",
      "Epoch 18/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0411 - acc: 0.9868 - val_loss: 0.0919 - val_acc: 0.9778\n",
      "Epoch 19/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0690 - acc: 0.9828 - val_loss: 0.0846 - val_acc: 0.9785\n",
      "Epoch 20/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0430 - acc: 0.9866 - val_loss: 0.1163 - val_acc: 0.9751\n",
      "Epoch 21/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0592 - acc: 0.9842 - val_loss: 0.0833 - val_acc: 0.9799\n",
      "Epoch 22/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0402 - acc: 0.9864 - val_loss: 0.1562 - val_acc: 0.9724\n",
      "Epoch 23/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0393 - acc: 0.9872 - val_loss: 0.0900 - val_acc: 0.9799\n",
      "Epoch 24/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0385 - acc: 0.9864 - val_loss: 0.0862 - val_acc: 0.9798\n",
      "Epoch 25/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0428 - acc: 0.9857 - val_loss: 0.1021 - val_acc: 0.9748\n",
      "Epoch 26/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0435 - acc: 0.9866 - val_loss: 0.0831 - val_acc: 0.9796\n",
      "Epoch 27/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0390 - acc: 0.9867 - val_loss: 0.0823 - val_acc: 0.9809\n",
      "Epoch 28/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0415 - acc: 0.9869 - val_loss: 0.0843 - val_acc: 0.9810\n",
      "Epoch 29/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0404 - acc: 0.9864 - val_loss: 0.0830 - val_acc: 0.9815\n",
      "Epoch 30/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0353 - acc: 0.9873 - val_loss: 0.1007 - val_acc: 0.9791\n",
      "Epoch 31/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0452 - acc: 0.9861 - val_loss: 0.0893 - val_acc: 0.9807\n",
      "Epoch 32/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0373 - acc: 0.9869 - val_loss: 0.0840 - val_acc: 0.9812\n",
      "Epoch 33/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0356 - acc: 0.9871 - val_loss: 0.1065 - val_acc: 0.9771\n",
      "Epoch 34/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0482 - acc: 0.9846 - val_loss: 0.0885 - val_acc: 0.9803\n",
      "Epoch 35/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0370 - acc: 0.9867 - val_loss: 0.0825 - val_acc: 0.9811\n",
      "Epoch 36/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0353 - acc: 0.9870 - val_loss: 0.0923 - val_acc: 0.9814\n",
      "Epoch 37/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0423 - acc: 0.9862 - val_loss: 0.0866 - val_acc: 0.9808\n",
      "Epoch 38/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0321 - acc: 0.9880 - val_loss: 0.0893 - val_acc: 0.9807\n",
      "Epoch 39/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0377 - acc: 0.9873 - val_loss: 0.0941 - val_acc: 0.9806\n",
      "Epoch 40/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0473 - acc: 0.9856 - val_loss: 0.0930 - val_acc: 0.9808\n",
      "Epoch 41/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0461 - acc: 0.9859 - val_loss: 0.0872 - val_acc: 0.9820\n",
      "Epoch 42/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0324 - acc: 0.9875 - val_loss: 0.0861 - val_acc: 0.9799\n",
      "Epoch 43/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0349 - acc: 0.9878 - val_loss: 0.0846 - val_acc: 0.9820\n",
      "Epoch 44/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0336 - acc: 0.9870 - val_loss: 0.0853 - val_acc: 0.9819\n",
      "Epoch 45/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0313 - acc: 0.9880 - val_loss: 0.0943 - val_acc: 0.9802\n",
      "Epoch 46/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0394 - acc: 0.9867 - val_loss: 0.0909 - val_acc: 0.9815\n",
      "Epoch 47/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0453 - acc: 0.9867 - val_loss: 0.0842 - val_acc: 0.9812\n",
      "Epoch 48/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0513 - acc: 0.9858 - val_loss: 0.0849 - val_acc: 0.9810\n",
      "Epoch 49/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0328 - acc: 0.9879 - val_loss: 0.1017 - val_acc: 0.9792\n",
      "Epoch 50/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0307 - acc: 0.9878 - val_loss: 0.0894 - val_acc: 0.9811\n",
      "Epoch 51/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0321 - acc: 0.9882 - val_loss: 0.0929 - val_acc: 0.9802\n",
      "Epoch 52/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0299 - acc: 0.9883 - val_loss: 0.1115 - val_acc: 0.9796\n",
      "Epoch 53/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0458 - acc: 0.9866 - val_loss: 0.0970 - val_acc: 0.9816\n",
      "Epoch 54/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0464 - acc: 0.9863 - val_loss: 0.0900 - val_acc: 0.9806\n",
      "Epoch 55/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0435 - acc: 0.9867 - val_loss: 0.0936 - val_acc: 0.9811\n",
      "Epoch 56/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0478 - acc: 0.9861 - val_loss: 0.0924 - val_acc: 0.9817\n",
      "Epoch 57/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0313 - acc: 0.9883 - val_loss: 0.0909 - val_acc: 0.9810\n",
      "Epoch 58/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0337 - acc: 0.9878 - val_loss: 0.0914 - val_acc: 0.9814\n",
      "Epoch 59/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0285 - acc: 0.9886 - val_loss: 0.0986 - val_acc: 0.9818\n",
      "Epoch 60/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0294 - acc: 0.9888 - val_loss: 0.1078 - val_acc: 0.9804\n",
      "Epoch 61/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0386 - acc: 0.9872 - val_loss: 0.0881 - val_acc: 0.9815\n",
      "Epoch 62/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0286 - acc: 0.9892 - val_loss: 0.0920 - val_acc: 0.9811\n",
      "Epoch 63/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0293 - acc: 0.9886 - val_loss: 0.0965 - val_acc: 0.9803\n",
      "Epoch 64/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0407 - acc: 0.9862 - val_loss: 0.0965 - val_acc: 0.9812\n",
      "Epoch 65/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0281 - acc: 0.9886 - val_loss: 0.0982 - val_acc: 0.9804\n",
      "Epoch 66/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0287 - acc: 0.9885 - val_loss: 0.0988 - val_acc: 0.9804\n",
      "Epoch 67/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0301 - acc: 0.9880 - val_loss: 0.1027 - val_acc: 0.9807\n",
      "Epoch 68/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0308 - acc: 0.9885 - val_loss: 0.1049 - val_acc: 0.9807\n",
      "Epoch 69/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0278 - acc: 0.9885 - val_loss: 0.1003 - val_acc: 0.9810\n",
      "Epoch 70/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0288 - acc: 0.9885 - val_loss: 0.0940 - val_acc: 0.9800\n",
      "Epoch 71/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0310 - acc: 0.9887 - val_loss: 0.0988 - val_acc: 0.9801\n",
      "Epoch 72/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0292 - acc: 0.9886 - val_loss: 0.1075 - val_acc: 0.9791\n",
      "Epoch 73/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0304 - acc: 0.9888 - val_loss: 0.1099 - val_acc: 0.9799\n",
      "Epoch 74/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0335 - acc: 0.9883 - val_loss: 0.0916 - val_acc: 0.9814\n",
      "Epoch 75/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0276 - acc: 0.9892 - val_loss: 0.1011 - val_acc: 0.9816\n",
      "Epoch 76/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0279 - acc: 0.9889 - val_loss: 0.0999 - val_acc: 0.9807\n",
      "Epoch 77/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0378 - acc: 0.9875 - val_loss: 0.1022 - val_acc: 0.9806\n",
      "Epoch 78/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0315 - acc: 0.9888 - val_loss: 0.0957 - val_acc: 0.9820\n",
      "Epoch 79/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0488 - acc: 0.9864 - val_loss: 0.0985 - val_acc: 0.9810\n",
      "Epoch 80/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0265 - acc: 0.9894 - val_loss: 0.1058 - val_acc: 0.9812\n",
      "Epoch 81/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0361 - acc: 0.9880 - val_loss: 0.0979 - val_acc: 0.9817\n",
      "Epoch 82/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0264 - acc: 0.9894 - val_loss: 0.0945 - val_acc: 0.9807\n",
      "Epoch 83/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0271 - acc: 0.9890 - val_loss: 0.0981 - val_acc: 0.9817\n",
      "Epoch 84/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0270 - acc: 0.9891 - val_loss: 0.0961 - val_acc: 0.9812\n",
      "Epoch 85/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0285 - acc: 0.9891 - val_loss: 0.1049 - val_acc: 0.9817\n",
      "Epoch 86/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0366 - acc: 0.9884 - val_loss: 0.1020 - val_acc: 0.9824\n",
      "Epoch 87/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0337 - acc: 0.9881 - val_loss: 0.0986 - val_acc: 0.9819\n",
      "Epoch 88/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0262 - acc: 0.9893 - val_loss: 0.0943 - val_acc: 0.9821\n",
      "Epoch 89/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0555 - acc: 0.9860 - val_loss: 0.1084 - val_acc: 0.9800\n",
      "Epoch 90/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0254 - acc: 0.9895 - val_loss: 0.1024 - val_acc: 0.9812\n",
      "Epoch 91/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0312 - acc: 0.9886 - val_loss: 0.0996 - val_acc: 0.9820\n",
      "Epoch 92/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0267 - acc: 0.9891 - val_loss: 0.1032 - val_acc: 0.9817\n",
      "Epoch 93/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0329 - acc: 0.9886 - val_loss: 0.1007 - val_acc: 0.9821\n",
      "Epoch 94/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0269 - acc: 0.9892 - val_loss: 0.3527 - val_acc: 0.9519\n",
      "Epoch 95/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0334 - acc: 0.9885 - val_loss: 0.0987 - val_acc: 0.9814\n",
      "Epoch 96/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0278 - acc: 0.9894 - val_loss: 0.0998 - val_acc: 0.9817\n",
      "Epoch 97/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0297 - acc: 0.9889 - val_loss: 0.0968 - val_acc: 0.9814\n",
      "Epoch 98/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0287 - acc: 0.9888 - val_loss: 0.0992 - val_acc: 0.9819\n",
      "Epoch 99/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0452 - acc: 0.9873 - val_loss: 0.1157 - val_acc: 0.9798\n",
      "Epoch 100/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0250 - acc: 0.9896 - val_loss: 0.0964 - val_acc: 0.9826\n",
      "Loading spacy...\n",
      "...done\n",
      "Sag: Ich möchte mit Kreditkarte bezahlen Frag: ein Ticket für Oliver Twist Sag: Ich möchte mit Dollars bezahlen Sag: Ich möchte mit Kreditkarte bezahlen\n"
     ]
    }
   ],
   "source": [
    "modelF = build_model((X.shape[1], X.shape[2]))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), nb_epoch=100, batch_size=256)\n",
    "save_model(model, 'mymodelF')\n",
    "loaded_model = load_model('output/mymodelF')\n",
    "predictor2 = Predictor(embedder, 'output/mymodelF', 'output/prompt_names.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(predictor2.predict('i want to pay with by credit card'),\n",
    "predictor2.predict('can i buy by post'),\n",
    "predictor2.predict('i would pay by dollars'),\n",
    "predictor2.predict('can i am a pay with credit card'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26408 samples, validate on 8794 samples\n",
      "Epoch 1/100\n",
      "26408/26408 [==============================] - 11s - loss: 4.6290 - acc: 0.0504 - val_loss: 4.1760 - val_acc: 0.0744\n",
      "Epoch 2/100\n",
      "26408/26408 [==============================] - 9s - loss: 3.0795 - acc: 0.1940 - val_loss: 2.8512 - val_acc: 0.2453\n",
      "Epoch 3/100\n",
      "26408/26408 [==============================] - 9s - loss: 2.0941 - acc: 0.4026 - val_loss: 2.2833 - val_acc: 0.3550\n",
      "Epoch 4/100\n",
      "26408/26408 [==============================] - 9s - loss: 1.3803 - acc: 0.6174 - val_loss: 1.5603 - val_acc: 0.5392\n",
      "Epoch 5/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.9501 - acc: 0.7576 - val_loss: 0.7155 - val_acc: 0.8407\n",
      "Epoch 6/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.7186 - acc: 0.8299 - val_loss: 0.5472 - val_acc: 0.8858\n",
      "Epoch 7/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.5509 - acc: 0.8783 - val_loss: 0.4539 - val_acc: 0.9055\n",
      "Epoch 8/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.5050 - acc: 0.8897 - val_loss: 0.3726 - val_acc: 0.9268\n",
      "Epoch 9/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.3810 - acc: 0.9173 - val_loss: 0.3359 - val_acc: 0.9312\n",
      "Epoch 10/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.3569 - acc: 0.9230 - val_loss: 0.3014 - val_acc: 0.9403\n",
      "Epoch 11/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.3688 - acc: 0.9251 - val_loss: 0.2689 - val_acc: 0.9499\n",
      "Epoch 12/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.2906 - acc: 0.9405 - val_loss: 0.2611 - val_acc: 0.9458\n",
      "Epoch 13/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.2798 - acc: 0.9424 - val_loss: 0.2281 - val_acc: 0.9578\n",
      "Epoch 14/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.2550 - acc: 0.9477 - val_loss: 0.2182 - val_acc: 0.9583\n",
      "Epoch 15/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.2220 - acc: 0.9530 - val_loss: 0.2121 - val_acc: 0.9593\n",
      "Epoch 16/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1971 - acc: 0.9624 - val_loss: 0.2099 - val_acc: 0.9579\n",
      "Epoch 17/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.2285 - acc: 0.9517 - val_loss: 0.1915 - val_acc: 0.9621\n",
      "Epoch 18/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.2178 - acc: 0.9589 - val_loss: 0.1821 - val_acc: 0.9635\n",
      "Epoch 19/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.1686 - acc: 0.9666 - val_loss: 0.5294 - val_acc: 0.9061\n",
      "Epoch 20/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.1711 - acc: 0.9636 - val_loss: 0.1777 - val_acc: 0.9670\n",
      "Epoch 21/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1734 - acc: 0.9655 - val_loss: 0.1615 - val_acc: 0.9663\n",
      "Epoch 22/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1426 - acc: 0.9694 - val_loss: 0.1587 - val_acc: 0.9662\n",
      "Epoch 23/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1526 - acc: 0.9672 - val_loss: 0.1563 - val_acc: 0.9710\n",
      "Epoch 24/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1275 - acc: 0.9723 - val_loss: 0.2310 - val_acc: 0.9486\n",
      "Epoch 25/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1632 - acc: 0.9688 - val_loss: 0.1482 - val_acc: 0.9702\n",
      "Epoch 26/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1352 - acc: 0.9699 - val_loss: 0.1745 - val_acc: 0.9596\n",
      "Epoch 27/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1183 - acc: 0.9738 - val_loss: 0.1475 - val_acc: 0.9719\n",
      "Epoch 28/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1284 - acc: 0.9731 - val_loss: 0.6743 - val_acc: 0.8666\n",
      "Epoch 29/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1154 - acc: 0.9749 - val_loss: 0.1402 - val_acc: 0.9735\n",
      "Epoch 30/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1015 - acc: 0.9765 - val_loss: 0.1319 - val_acc: 0.9733\n",
      "Epoch 31/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1341 - acc: 0.9712 - val_loss: 0.1313 - val_acc: 0.9734\n",
      "Epoch 32/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0980 - acc: 0.9773 - val_loss: 0.1373 - val_acc: 0.9738\n",
      "Epoch 33/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0983 - acc: 0.9793 - val_loss: 0.1348 - val_acc: 0.9730\n",
      "Epoch 34/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.1086 - acc: 0.9777 - val_loss: 0.1296 - val_acc: 0.9733\n",
      "Epoch 35/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0904 - acc: 0.9787 - val_loss: 0.1277 - val_acc: 0.9753\n",
      "Epoch 36/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0949 - acc: 0.9791 - val_loss: 0.1505 - val_acc: 0.9692\n",
      "Epoch 37/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0714 - acc: 0.9814 - val_loss: 0.1225 - val_acc: 0.9750\n",
      "Epoch 38/100\n",
      "26408/26408 [==============================] - 8s - loss: 0.0807 - acc: 0.9801 - val_loss: 0.1218 - val_acc: 0.9767\n",
      "Epoch 39/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0755 - acc: 0.9816 - val_loss: 0.2066 - val_acc: 0.9617\n",
      "Epoch 40/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0754 - acc: 0.9821 - val_loss: 0.1109 - val_acc: 0.9781\n",
      "Epoch 41/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0682 - acc: 0.9826 - val_loss: 0.1287 - val_acc: 0.9733\n",
      "Epoch 42/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0877 - acc: 0.9802 - val_loss: 0.1147 - val_acc: 0.9776\n",
      "Epoch 43/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0717 - acc: 0.9826 - val_loss: 0.1090 - val_acc: 0.9776\n",
      "Epoch 44/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0629 - acc: 0.9845 - val_loss: 0.1112 - val_acc: 0.9761\n",
      "Epoch 45/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0548 - acc: 0.9850 - val_loss: 0.1102 - val_acc: 0.9783\n",
      "Epoch 46/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0557 - acc: 0.9855 - val_loss: 0.1246 - val_acc: 0.9736\n",
      "Epoch 47/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0666 - acc: 0.9832 - val_loss: 0.1056 - val_acc: 0.9775\n",
      "Epoch 48/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0723 - acc: 0.9824 - val_loss: 0.1013 - val_acc: 0.9784\n",
      "Epoch 49/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0536 - acc: 0.9850 - val_loss: 0.1088 - val_acc: 0.9783\n",
      "Epoch 50/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0651 - acc: 0.9837 - val_loss: 0.1496 - val_acc: 0.9750\n",
      "Epoch 51/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0711 - acc: 0.9827 - val_loss: 0.1687 - val_acc: 0.9702\n",
      "Epoch 52/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0519 - acc: 0.9858 - val_loss: 0.1307 - val_acc: 0.9712\n",
      "Epoch 53/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0527 - acc: 0.9850 - val_loss: 0.1035 - val_acc: 0.9788\n",
      "Epoch 54/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0723 - acc: 0.9834 - val_loss: 0.1063 - val_acc: 0.9782\n",
      "Epoch 55/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0467 - acc: 0.9864 - val_loss: 0.1110 - val_acc: 0.9785\n",
      "Epoch 56/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0569 - acc: 0.9847 - val_loss: 0.1021 - val_acc: 0.9778\n",
      "Epoch 57/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0619 - acc: 0.9841 - val_loss: 0.1053 - val_acc: 0.9790\n",
      "Epoch 58/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0507 - acc: 0.9859 - val_loss: 0.1057 - val_acc: 0.9783\n",
      "Epoch 59/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0686 - acc: 0.9838 - val_loss: 0.1079 - val_acc: 0.9779\n",
      "Epoch 60/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0478 - acc: 0.9866 - val_loss: 0.1038 - val_acc: 0.9790\n",
      "Epoch 61/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0623 - acc: 0.9839 - val_loss: 0.1067 - val_acc: 0.9783\n",
      "Epoch 62/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0521 - acc: 0.9854 - val_loss: 0.1584 - val_acc: 0.9684\n",
      "Epoch 63/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0529 - acc: 0.9852 - val_loss: 0.1036 - val_acc: 0.9784\n",
      "Epoch 64/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0573 - acc: 0.9846 - val_loss: 0.1024 - val_acc: 0.9788\n",
      "Epoch 65/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0433 - acc: 0.9868 - val_loss: 0.0998 - val_acc: 0.9785\n",
      "Epoch 66/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0483 - acc: 0.9860 - val_loss: 0.1037 - val_acc: 0.9787\n",
      "Epoch 67/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0533 - acc: 0.9855 - val_loss: 0.1057 - val_acc: 0.9771\n",
      "Epoch 68/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0417 - acc: 0.9875 - val_loss: 0.1676 - val_acc: 0.9730\n",
      "Epoch 69/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0434 - acc: 0.9869 - val_loss: 0.1022 - val_acc: 0.9796\n",
      "Epoch 70/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0700 - acc: 0.9841 - val_loss: 0.0999 - val_acc: 0.9792\n",
      "Epoch 71/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0418 - acc: 0.9872 - val_loss: 0.1065 - val_acc: 0.9776\n",
      "Epoch 72/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0426 - acc: 0.9874 - val_loss: 0.1077 - val_acc: 0.9799\n",
      "Epoch 73/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0498 - acc: 0.9857 - val_loss: 0.1908 - val_acc: 0.9608\n",
      "Epoch 74/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0420 - acc: 0.9878 - val_loss: 0.1181 - val_acc: 0.9771\n",
      "Epoch 75/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0522 - acc: 0.9859 - val_loss: 0.0993 - val_acc: 0.9798\n",
      "Epoch 76/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0398 - acc: 0.9875 - val_loss: 0.1100 - val_acc: 0.9796\n",
      "Epoch 77/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0402 - acc: 0.9875 - val_loss: 0.1006 - val_acc: 0.9799\n",
      "Epoch 78/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0401 - acc: 0.9876 - val_loss: 0.1076 - val_acc: 0.9803\n",
      "Epoch 79/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0498 - acc: 0.9857 - val_loss: 0.1142 - val_acc: 0.9783\n",
      "Epoch 80/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0387 - acc: 0.9877 - val_loss: 0.1090 - val_acc: 0.9785\n",
      "Epoch 81/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0424 - acc: 0.9872 - val_loss: 0.1302 - val_acc: 0.9730\n",
      "Epoch 82/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0406 - acc: 0.9875 - val_loss: 0.0965 - val_acc: 0.9812\n",
      "Epoch 83/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0386 - acc: 0.9881 - val_loss: 0.1041 - val_acc: 0.9799\n",
      "Epoch 84/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0658 - acc: 0.9853 - val_loss: 0.1009 - val_acc: 0.9803\n",
      "Epoch 85/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0386 - acc: 0.9881 - val_loss: 0.1084 - val_acc: 0.9798\n",
      "Epoch 86/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0431 - acc: 0.9870 - val_loss: 0.1016 - val_acc: 0.9798\n",
      "Epoch 87/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0387 - acc: 0.9885 - val_loss: 0.1073 - val_acc: 0.9799\n",
      "Epoch 88/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0496 - acc: 0.9865 - val_loss: 0.1309 - val_acc: 0.9738\n",
      "Epoch 89/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0516 - acc: 0.9860 - val_loss: 0.1249 - val_acc: 0.9762\n",
      "Epoch 90/100\n",
      "26408/26408 [==============================] - 10s - loss: 0.0380 - acc: 0.9881 - val_loss: 0.1078 - val_acc: 0.9802\n",
      "Epoch 91/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0395 - acc: 0.9873 - val_loss: 0.0971 - val_acc: 0.9812\n",
      "Epoch 92/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0411 - acc: 0.9872 - val_loss: 0.1080 - val_acc: 0.9806\n",
      "Epoch 93/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0383 - acc: 0.9879 - val_loss: 0.1045 - val_acc: 0.9804\n",
      "Epoch 94/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0503 - acc: 0.9865 - val_loss: 0.1073 - val_acc: 0.9795\n",
      "Epoch 95/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0365 - acc: 0.9886 - val_loss: 0.1089 - val_acc: 0.9795\n",
      "Epoch 96/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0379 - acc: 0.9881 - val_loss: 0.1067 - val_acc: 0.9800\n",
      "Epoch 97/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0412 - acc: 0.9879 - val_loss: 0.1073 - val_acc: 0.9806\n",
      "Epoch 98/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0378 - acc: 0.9885 - val_loss: 0.1072 - val_acc: 0.9807\n",
      "Epoch 99/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0467 - acc: 0.9872 - val_loss: 0.1054 - val_acc: 0.9806\n",
      "Epoch 100/100\n",
      "26408/26408 [==============================] - 9s - loss: 0.0373 - acc: 0.9883 - val_loss: 0.1076 - val_acc: 0.9798\n"
     ]
    }
   ],
   "source": [
    "model3 = build_model3((X.shape[1], X.shape[2]))\n",
    "history = model3.fit(X_train, y_train, validation_data=(X_dev, y_dev), nb_epoch=100, batch_size=256)\n",
    "save_model(model3, 'model3')\n",
    "loaded_model = load_model('output/model3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor3 = Predictor(embedder, 'output/model3', 'output/prompt_names.p')\n",
    "print(predictor3.predict('i want to pay with by credit card'),\n",
    "predictor3.predict('can i buy by post'),\n",
    "predictor3.predict('i would pay by dollars'),\n",
    "predictor3.predict('can i am a pay with credit card'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26408 samples, validate on 8794 samples\n",
      "Epoch 1/100\n",
      "26408/26408 [==============================] - 12s - loss: 4.5833 - acc: 0.0527 - val_loss: 3.7654 - val_acc: 0.0930\n",
      "Epoch 2/100\n",
      "26408/26408 [==============================] - 11s - loss: 2.8122 - acc: 0.2356 - val_loss: 2.5385 - val_acc: 0.3073\n",
      "Epoch 3/100\n",
      "26408/26408 [==============================] - 11s - loss: 1.7698 - acc: 0.4830 - val_loss: 1.5437 - val_acc: 0.5889\n",
      "Epoch 4/100\n",
      "26408/26408 [==============================] - 12s - loss: 1.0456 - acc: 0.7327 - val_loss: 1.3984 - val_acc: 0.6491\n",
      "Epoch 5/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.7663 - acc: 0.8223 - val_loss: 1.1758 - val_acc: 0.6674\n",
      "Epoch 6/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.5661 - acc: 0.8720 - val_loss: 1.2474 - val_acc: 0.6301\n",
      "Epoch 7/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.4397 - acc: 0.9048 - val_loss: 0.3479 - val_acc: 0.9318\n",
      "Epoch 8/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.4201 - acc: 0.9110 - val_loss: 0.3081 - val_acc: 0.9378\n",
      "Epoch 9/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.3362 - acc: 0.9316 - val_loss: 0.2658 - val_acc: 0.9496\n",
      "Epoch 10/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.2977 - acc: 0.9398 - val_loss: 0.2493 - val_acc: 0.9526\n",
      "Epoch 11/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.2591 - acc: 0.9468 - val_loss: 0.2230 - val_acc: 0.9596\n",
      "Epoch 12/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.2567 - acc: 0.9481 - val_loss: 0.2142 - val_acc: 0.9579\n",
      "Epoch 13/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.1995 - acc: 0.9620 - val_loss: 0.2522 - val_acc: 0.9485\n",
      "Epoch 14/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.2057 - acc: 0.9582 - val_loss: 0.1946 - val_acc: 0.9622\n",
      "Epoch 15/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.2336 - acc: 0.9542 - val_loss: 0.1774 - val_acc: 0.9650\n",
      "Epoch 16/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1685 - acc: 0.9679 - val_loss: 0.1713 - val_acc: 0.9640\n",
      "Epoch 17/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1728 - acc: 0.9634 - val_loss: 0.1577 - val_acc: 0.9683\n",
      "Epoch 18/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1303 - acc: 0.9709 - val_loss: 0.1541 - val_acc: 0.9674\n",
      "Epoch 19/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1695 - acc: 0.9639 - val_loss: 0.1509 - val_acc: 0.9688\n",
      "Epoch 20/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1182 - acc: 0.9728 - val_loss: 0.1687 - val_acc: 0.9619\n",
      "Epoch 21/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1174 - acc: 0.9732 - val_loss: 0.2451 - val_acc: 0.9403\n",
      "Epoch 22/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1298 - acc: 0.9721 - val_loss: 0.1369 - val_acc: 0.9702\n",
      "Epoch 23/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1457 - acc: 0.9697 - val_loss: 0.2189 - val_acc: 0.9494\n",
      "Epoch 24/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1157 - acc: 0.9750 - val_loss: 0.1370 - val_acc: 0.9705\n",
      "Epoch 25/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1093 - acc: 0.9756 - val_loss: 0.4992 - val_acc: 0.9089\n",
      "Epoch 26/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.1193 - acc: 0.9726 - val_loss: 0.1126 - val_acc: 0.9746\n",
      "Epoch 27/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.1144 - acc: 0.9750 - val_loss: 0.1160 - val_acc: 0.9732\n",
      "Epoch 28/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.1260 - acc: 0.9722 - val_loss: 0.1164 - val_acc: 0.9745\n",
      "Epoch 29/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0958 - acc: 0.9778 - val_loss: 0.1121 - val_acc: 0.9740\n",
      "Epoch 30/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0750 - acc: 0.9807 - val_loss: 0.1072 - val_acc: 0.9756\n",
      "Epoch 31/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0776 - acc: 0.9808 - val_loss: 0.1224 - val_acc: 0.9745\n",
      "Epoch 32/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0830 - acc: 0.9803 - val_loss: 0.1338 - val_acc: 0.9700\n",
      "Epoch 33/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.1135 - acc: 0.9744 - val_loss: 0.1007 - val_acc: 0.9756\n",
      "Epoch 34/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0794 - acc: 0.9817 - val_loss: 0.0978 - val_acc: 0.9776\n",
      "Epoch 35/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0732 - acc: 0.9800 - val_loss: 0.1001 - val_acc: 0.9778\n",
      "Epoch 36/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0813 - acc: 0.9819 - val_loss: 0.0930 - val_acc: 0.9781\n",
      "Epoch 37/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0647 - acc: 0.9826 - val_loss: 0.1901 - val_acc: 0.9616\n",
      "Epoch 38/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0570 - acc: 0.9841 - val_loss: 0.1337 - val_acc: 0.9663\n",
      "Epoch 39/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0697 - acc: 0.9818 - val_loss: 0.0930 - val_acc: 0.9763\n",
      "Epoch 40/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0664 - acc: 0.9830 - val_loss: 0.0891 - val_acc: 0.9774\n",
      "Epoch 41/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0688 - acc: 0.9825 - val_loss: 0.0918 - val_acc: 0.9767\n",
      "Epoch 42/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0500 - acc: 0.9847 - val_loss: 0.0897 - val_acc: 0.9771\n",
      "Epoch 43/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0775 - acc: 0.9828 - val_loss: 0.0887 - val_acc: 0.9777\n",
      "Epoch 44/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0577 - acc: 0.9835 - val_loss: 0.0951 - val_acc: 0.9777\n",
      "Epoch 45/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0498 - acc: 0.9854 - val_loss: 0.0937 - val_acc: 0.9778\n",
      "Epoch 46/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0643 - acc: 0.9820 - val_loss: 0.0834 - val_acc: 0.9796\n",
      "Epoch 47/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0566 - acc: 0.9833 - val_loss: 0.0879 - val_acc: 0.9781\n",
      "Epoch 48/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0482 - acc: 0.9856 - val_loss: 0.0873 - val_acc: 0.9784\n",
      "Epoch 49/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0439 - acc: 0.9860 - val_loss: 0.0840 - val_acc: 0.9785\n",
      "Epoch 50/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0651 - acc: 0.9832 - val_loss: 0.0986 - val_acc: 0.9783\n",
      "Epoch 51/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0476 - acc: 0.9849 - val_loss: 0.0874 - val_acc: 0.9788\n",
      "Epoch 52/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0399 - acc: 0.9863 - val_loss: 0.0861 - val_acc: 0.9795\n",
      "Epoch 53/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0496 - acc: 0.9846 - val_loss: 0.0975 - val_acc: 0.9771\n",
      "Epoch 54/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0392 - acc: 0.9869 - val_loss: 0.0869 - val_acc: 0.9799\n",
      "Epoch 55/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0417 - acc: 0.9864 - val_loss: 0.3039 - val_acc: 0.9409\n",
      "Epoch 56/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0482 - acc: 0.9853 - val_loss: 0.0792 - val_acc: 0.9794\n",
      "Epoch 57/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0442 - acc: 0.9855 - val_loss: 0.0941 - val_acc: 0.9777\n",
      "Epoch 58/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0423 - acc: 0.9858 - val_loss: 0.0839 - val_acc: 0.9785\n",
      "Epoch 59/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0402 - acc: 0.9864 - val_loss: 0.0840 - val_acc: 0.9798\n",
      "Epoch 60/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0458 - acc: 0.9846 - val_loss: 0.0847 - val_acc: 0.9796\n",
      "Epoch 61/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0392 - acc: 0.9863 - val_loss: 0.0839 - val_acc: 0.9804\n",
      "Epoch 62/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0363 - acc: 0.9874 - val_loss: 0.0805 - val_acc: 0.9801\n",
      "Epoch 63/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0410 - acc: 0.9866 - val_loss: 0.0862 - val_acc: 0.9793\n",
      "Epoch 64/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0385 - acc: 0.9861 - val_loss: 0.0909 - val_acc: 0.9778\n",
      "Epoch 65/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0526 - acc: 0.9854 - val_loss: 0.0863 - val_acc: 0.9806\n",
      "Epoch 66/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0366 - acc: 0.9870 - val_loss: 0.0965 - val_acc: 0.9771\n",
      "Epoch 67/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0340 - acc: 0.9875 - val_loss: 0.0913 - val_acc: 0.9781\n",
      "Epoch 68/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0340 - acc: 0.9881 - val_loss: 0.0936 - val_acc: 0.9803\n",
      "Epoch 69/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0371 - acc: 0.9872 - val_loss: 0.1068 - val_acc: 0.9762\n",
      "Epoch 70/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0473 - acc: 0.9862 - val_loss: 0.0852 - val_acc: 0.9800\n",
      "Epoch 71/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0319 - acc: 0.9883 - val_loss: 0.0938 - val_acc: 0.9800\n",
      "Epoch 72/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0422 - acc: 0.9856 - val_loss: 0.0820 - val_acc: 0.9810\n",
      "Epoch 73/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0440 - acc: 0.9866 - val_loss: 0.0834 - val_acc: 0.9811\n",
      "Epoch 74/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0320 - acc: 0.9878 - val_loss: 0.0839 - val_acc: 0.9812\n",
      "Epoch 75/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0322 - acc: 0.9878 - val_loss: 0.0841 - val_acc: 0.9814\n",
      "Epoch 76/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0462 - acc: 0.9868 - val_loss: 0.0808 - val_acc: 0.9811\n",
      "Epoch 77/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0559 - acc: 0.9859 - val_loss: 0.0932 - val_acc: 0.9821\n",
      "Epoch 78/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0325 - acc: 0.9877 - val_loss: 0.0820 - val_acc: 0.9798\n",
      "Epoch 79/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0434 - acc: 0.9865 - val_loss: 0.0825 - val_acc: 0.9811\n",
      "Epoch 80/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0337 - acc: 0.9876 - val_loss: 0.0811 - val_acc: 0.9807\n",
      "Epoch 81/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0323 - acc: 0.9884 - val_loss: 0.2201 - val_acc: 0.9624\n",
      "Epoch 82/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0349 - acc: 0.9883 - val_loss: 0.0881 - val_acc: 0.9808\n",
      "Epoch 83/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0400 - acc: 0.9868 - val_loss: 0.5388 - val_acc: 0.9320\n",
      "Epoch 84/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0324 - acc: 0.9883 - val_loss: 0.0955 - val_acc: 0.9809\n",
      "Epoch 85/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0328 - acc: 0.9878 - val_loss: 0.0884 - val_acc: 0.9816\n",
      "Epoch 86/100\n",
      "26408/26408 [==============================] - 12s - loss: 0.0304 - acc: 0.9884 - val_loss: 0.0946 - val_acc: 0.9817\n",
      "Epoch 87/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0297 - acc: 0.9885 - val_loss: 0.1020 - val_acc: 0.9795\n",
      "Epoch 88/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0318 - acc: 0.9886 - val_loss: 0.0892 - val_acc: 0.9806\n",
      "Epoch 89/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0352 - acc: 0.9875 - val_loss: 0.0934 - val_acc: 0.9811\n",
      "Epoch 90/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0500 - acc: 0.9865 - val_loss: 0.0829 - val_acc: 0.9807\n",
      "Epoch 91/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0409 - acc: 0.9879 - val_loss: 0.0961 - val_acc: 0.9807\n",
      "Epoch 92/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0306 - acc: 0.9886 - val_loss: 0.0837 - val_acc: 0.9823\n",
      "Epoch 93/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0428 - acc: 0.9875 - val_loss: 0.0950 - val_acc: 0.9821\n",
      "Epoch 94/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0283 - acc: 0.9886 - val_loss: 0.0913 - val_acc: 0.9819\n",
      "Epoch 95/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0291 - acc: 0.9886 - val_loss: 0.0927 - val_acc: 0.9823\n",
      "Epoch 96/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0350 - acc: 0.9873 - val_loss: 0.0912 - val_acc: 0.9814\n",
      "Epoch 97/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0336 - acc: 0.9881 - val_loss: 0.0867 - val_acc: 0.9819\n",
      "Epoch 98/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0291 - acc: 0.9885 - val_loss: 0.0897 - val_acc: 0.9810\n",
      "Epoch 99/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0342 - acc: 0.9883 - val_loss: 0.0930 - val_acc: 0.9820\n",
      "Epoch 100/100\n",
      "26408/26408 [==============================] - 11s - loss: 0.0293 - acc: 0.9885 - val_loss: 0.1081 - val_acc: 0.9791\n"
     ]
    }
   ],
   "source": [
    "model4 = build_model2((X.shape[1], X.shape[2]))\n",
    "history = model4.fit(X_train, y_train, validation_data=(X_dev, y_dev), nb_epoch=100, batch_size=256)\n",
    "save_model(model4, 'model4')\n",
    "loaded_model = load_model('output/model4')\n",
    "predictor4 = Predictor(embedder, 'output/model4', 'output/prompt_names.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sag: Ich möchte mit Kreditkarte bezahlen Sag: Ich möchte mit Visa bezahlen Sag: Ich möchte mit Dollars bezahlen Sag: Ich möchte mit Kreditkarte bezahlen\n"
     ]
    }
   ],
   "source": [
    "print(predictor4.predict('i want to pay with by credit card'),\n",
    "predictor4.predict('can i buy by post'),\n",
    "predictor4.predict('i would pay by dollars'),\n",
    "predictor4.predict('can i am a pay with credit card'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Model on ST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaldi = open('data/st_data/kaldi_input', 'r')\n",
    "out = open('output/predicted_meaning', 'a')\n",
    "for line in kaldi:\n",
    "    out.write(predictor.predict(line) + '\\n')\n",
    "    \n",
    "kaldi.close()\n",
    "out.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
